{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `extract_context_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_words(text, nlp, context_pos=('NOUN', 'VERB', 'ADJ')):\n",
    "    \"\"\"입력된 text로부터 명사/동사 추출\n",
    "    \n",
    "    Args:\n",
    "        text (str): Context 추출할 텍스트\n",
    "        nlp (spacy.lang.en.English): spaCy 모델\n",
    "        context_pos (list): 추출할 품사\n",
    "    Returns:\n",
    "        list: 텍스트에서 추출한 문맥 단어 리스트. ['단어 소문자'] 형태. stopword 제외\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "#     return [(tok.text.lower(), tok.pos_) for tok in doc if tok.pos_ in ('NOUN', 'VERB')]\n",
    "    return [tok.lemma_.lower() for tok in doc if tok.pos_ in context_pos and not tok.is_stop]\n",
    "#     return [tok.text.lower() for tok in doc if not tok.is_stop and tok.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization',\n",
       " 'big',\n",
       " 'standard',\n",
       " 'base',\n",
       " 'corpus',\n",
       " 'tokenizer',\n",
       " 'differ',\n",
       " 'include',\n",
       " 'token',\n",
       " 'significant',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Tokenization big standards are based on the OntoNotes 5 corpus. The tokenizer differs from most by including tokens for significant whitespace.'\n",
    "nlp = spacy.load('en')\n",
    "extract_context_words(text, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_word2vec`, `load_nnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec():\n",
    "    \"\"\"word2vec 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('./data/embeddings/word2vec_300.parquet')\n",
    "    return embeddings\n",
    "\n",
    "def load_nnse():\n",
    "    \"\"\"NNSE 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('./data/embeddings/nnse_2500.parquet')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d291</th>\n",
       "      <th>d292</th>\n",
       "      <th>d293</th>\n",
       "      <th>d294</th>\n",
       "      <th>d295</th>\n",
       "      <th>d296</th>\n",
       "      <th>d297</th>\n",
       "      <th>d298</th>\n",
       "      <th>d299</th>\n",
       "      <th>d300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.050293</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.026733</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.070801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            d1        d2        d3        d4        d5        d6        d7  \\\n",
       "word                                                                         \n",
       "in    0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "for  -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "that -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
       "is    0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "on    0.026733 -0.090820  0.027832  0.204102  0.006226 -0.090332  0.022583   \n",
       "\n",
       "            d8        d9       d10    ...         d291      d292      d293  \\\n",
       "word                                  ...                                    \n",
       "in   -0.154297  0.020752  0.131836    ...    -0.168945 -0.088867 -0.080566   \n",
       "for  -0.072266  0.064453  0.086426    ...    -0.022583  0.003723 -0.082520   \n",
       "that -0.089844  0.117676  0.118164    ...    -0.011292 -0.015625 -0.033447   \n",
       "is   -0.107910  0.071777  0.020874    ...    -0.233398 -0.036377 -0.093750   \n",
       "on   -0.161133  0.132812  0.061035    ...     0.026855 -0.027954  0.030884   \n",
       "\n",
       "          d294      d295      d296      d297      d298      d299      d300  \n",
       "word                                                                        \n",
       "in    0.064941  0.061279 -0.047363 -0.058838 -0.047607  0.014465 -0.062500  \n",
       "for   0.081543  0.007935  0.000477  0.018433  0.071289 -0.034912  0.024170  \n",
       "that -0.020630 -0.019409  0.063965  0.020142  0.006866  0.061035 -0.148438  \n",
       "is    0.182617  0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934  \n",
       "on    0.040527 -0.130859  0.083008  0.015747 -0.116699 -0.029419 -0.070801  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_word2vec = load_word2vec()\n",
    "emb_nnse = load_nnse()\n",
    "emb_word2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'for', 'that', 'is', 'on']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = 단어\n",
    "emb_word2vec.head().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_embeddings_for_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_words(words, embeddings):\n",
    "    \"\"\"주어진 단어들의 임베딩을 `pandas.DataFrame` 형태로 반환\n",
    "    \n",
    "    Args:\n",
    "        words (list): 단어 리스트\n",
    "        embeddings (pandas.DataFrame): 임베딩 행렬 (`load_word2vec`, `load_nnse` 반환 형태)\n",
    "    Returns:\n",
    "        pandas.DataFrame: 주어진 단어들의 임베딩\n",
    "        \n",
    "        단어가 임베딩 행렬에 없는 경우 제외\n",
    "    \"\"\"\n",
    "    return embeddings.loc[embeddings.index.intersection(words)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d2491</th>\n",
       "      <th>d2492</th>\n",
       "      <th>d2493</th>\n",
       "      <th>d2494</th>\n",
       "      <th>d2495</th>\n",
       "      <th>d2496</th>\n",
       "      <th>d2497</th>\n",
       "      <th>d2498</th>\n",
       "      <th>d2499</th>\n",
       "      <th>d2500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standards</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>differs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            d1   d2   d3   d4   d5   d6   d7   d8   d9  d10  ...    d2491  \\\n",
       "including  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "standards  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "differs    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "\n",
       "           d2492  d2493  d2494  d2495  d2496  d2497  d2498  d2499  d2500  \n",
       "including    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "standards    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "differs      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 2500 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['standards', 'differs', 'including', 'abcc'] # 'abcc'는 행렬에 없음\n",
    "word_embeddings = get_embeddings_for_words(words, emb_nnse)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d2491</th>\n",
       "      <th>d2492</th>\n",
       "      <th>d2493</th>\n",
       "      <th>d2494</th>\n",
       "      <th>d2495</th>\n",
       "      <th>d2496</th>\n",
       "      <th>d2497</th>\n",
       "      <th>d2498</th>\n",
       "      <th>d2499</th>\n",
       "      <th>d2500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        d1   d2   d3   d4   d5   d6   d7   d8   d9  d10  ...    d2491  d2492  \\\n",
       "apple  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "\n",
       "       d2493  d2494  d2495  d2496  d2497  d2498  d2499  d2500  \n",
       "apple    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[1 rows x 2500 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = get_embeddings_for_words(['apple'], emb_nnse)\n",
    "apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d2491</th>\n",
       "      <th>d2492</th>\n",
       "      <th>d2493</th>\n",
       "      <th>d2494</th>\n",
       "      <th>d2495</th>\n",
       "      <th>d2496</th>\n",
       "      <th>d2497</th>\n",
       "      <th>d2498</th>\n",
       "      <th>d2499</th>\n",
       "      <th>d2500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           d1   d2   d3   d4   d5   d6   d7   d8   d9  d10  ...    d2491  \\\n",
       "computer  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "tech      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "company   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "\n",
       "          d2492  d2493  d2494  d2495  d2496  d2497  d2498  d2499  d2500  \n",
       "computer    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "tech        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "company     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 2500 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = get_embeddings_for_words(['computer', 'tech', 'company'], emb_nnse)\n",
    "apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `explain_dim`, `explain_dims`, `get_sig_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_cache = {}\n",
    "def _explain_dim(index, emb, embeddings):\n",
    "    k = 5\n",
    "    col = 'd{}'.format(index+1)\n",
    "    if not col in desc_cache:\n",
    "        desc_cache[col] = embeddings.sort_values(by=col, ascending=False).index.tolist()[:k]\n",
    "    desc =  desc_cache[col]\n",
    "    return (col, ', '.join(desc), emb[index])\n",
    "\n",
    "def explain_dims(indices, emb, embeddings, k=5):\n",
    "    results = [_explain_dim(i, emb, embeddings) for i in indices]\n",
    "    results = sorted(results, key=lambda item: item[2], reverse=True)\n",
    "    results = [(item[0], item[1], '{:.5f}'.format(item[2])) for item in results]\n",
    "    return results\n",
    "\n",
    "def get_sig_dims(emb, thres=0.01):\n",
    "    \"\"\"값이 `thres` 이상인 차원 index 반환\n",
    "    \n",
    "    Args:\n",
    "        emb (numpy.array): 차원을 추출할 임베딩. 1d array.\n",
    "    Returns:\n",
    "        numpy.array: sig 차원이 표시된 mask (예: array([False, True, False, ...]))\n",
    "    \"\"\"\n",
    "    if len(emb.shape) > 1:\n",
    "        raise ValueError('`emb` argument should be 1D array')\n",
    "    if type(thres) != float:\n",
    "        raise ValueError('`float` argument should be float')\n",
    "    \n",
    "    return np.where((emb > thres) == True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d1694', 'seller, specifics, ebay, item, bid', '0.66279'),\n",
       " ('d1578', 'bids, valid, matching, enlarge, ebay', '0.49128'),\n",
       " ('d1257', 'antiques, auction, auctions, collectibles, antique', '0.13379'),\n",
       " ('d639', 'helsinki, dashes, weber, fraser, booksellers', '0.08437'),\n",
       " ('d579', 'quot, amp, lt, wal, gt', '0.06565'),\n",
       " ('d40', 'retail, kroger, kmart, safeway, wholesale', '0.03290'),\n",
       " ('d2398', 'thrift, duty-free, penney, second-hand, souvenir', '0.02616'),\n",
       " ('d1209', 'selling, buying, purchasing, sell, sells', '0.01678'),\n",
       " ('d1233', 'nike, dunk, adidas, sneaker, converse', '0.01630'),\n",
       " ('d2329', 'vous, sur, les, par, le', '0.01574'),\n",
       " ('d865', 'rated, rating, ratings, visitor, sort', '0.01328'),\n",
       " ('d2110', 'watchlist, drafts, artists, labels, submissions', '0.01228')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = get_embeddings_for_words(['ebay'], emb_nnse).values.flatten()\n",
    "dims = get_sig_dims(emb)\n",
    "explain_dims(dims, emb, emb_nnse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_sum` (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_embeddings_sum(target_embs, context_embs):\n",
    "    \"\"\"[Baseline] 주어진 단어 임베딩들의 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    단순 덧셈\n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (덧셈). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    # 임베딩 행렬 복제본에 작업\n",
    "    target_embs = target_embs.copy()\n",
    "    context_embs = context_embs.copy()\n",
    "    \n",
    "    embs = pd.concat([target_embs, context_embs])\n",
    "    return embs.sum(axis=0).values.reshape(1, -1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합성 결과물 shape = (1, #_of_dimensions)\n",
    "compose_embeddings_sum(word_embeddings.iloc[1:], word_embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_reactive` (My)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    \"\"\"\n",
    "    Calculate the softmax for the give inputs (array)\n",
    "    :param inputs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.exp(inputs) / float(sum(np.exp(inputs)))\n",
    "\n",
    "DEBUG = False\n",
    "def compose_embeddings_reactive(target_embs, context_embs):\n",
    "    \"\"\"[Proposing] 주어진 단어 임베딩들의 contextualized 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    1. \n",
    "    2. \n",
    "    3. \n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (Contextualized). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    if type(target_embs) != pd.core.frame.DataFrame and type(context_embs) != pd.core.frame.DataFrame:\n",
    "        raise ValueError('target_embs and context_embs must be DataFrame')\n",
    "    \n",
    "    # 임베딩 행렬 복제본에 작업 (원본 행렬 유지)\n",
    "    target_embs = target_embs.copy()\n",
    "    context_embs = context_embs.copy()\n",
    "    \n",
    "    # 임베딩을 1차원 벡터로 변환 (context는 먼저 합친 후 변환)\n",
    "    target = target_embs.sum().values\n",
    "    context = context_embs.sum().values\n",
    "    \n",
    "    # target*context\n",
    "    target = np.multiply(target, context)\n",
    "    \n",
    "    # deactivate weak dimensions\n",
    "#     thres = 0.001\n",
    "#     weak_dims = target < thres\n",
    "#     target[weak_dims] = 0.0\n",
    "\n",
    "    # 반환값\n",
    "    result = normalize(target.reshape(1, -1))\n",
    "#     result = target.reshape(1, -1)\n",
    "\n",
    "    # 디버깅\n",
    "    if DEBUG:\n",
    "        print('[Words]', ', '.join(target_embs.index.tolist() + context_embs.index.tolist()))\n",
    "        explain = explain_dims(result.nonzero()[1], result.flatten(), emb_nnse)\n",
    "        pprint(explain)\n",
    "\n",
    "#     return target.reshape(1, -1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_embeddings_for_words(['apple'], emb_nnse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Words] apple, fast, latest, electronics\n",
      "[('d1724', 'aac, ogg, rm, ripper, converter', '0.96822'),\n",
      " ('d2104', 'alcatel, lg, motorola, samsung, sony', '0.24150'),\n",
      " ('d872', 'bea, microsoft, enterprise, ria, oracle', '0.06416'),\n",
      " ('d1778', 'cool, crazy, gadget, animation, sexy', '0.00771'),\n",
      " ('d1478', 'amoeba, rampage, shootout, buster, swat', '0.00678')]\n"
     ]
    }
   ],
   "source": [
    "context = get_embeddings_for_words(['electronics', 'latest', 'fast'], emb_nnse)\n",
    "\n",
    "res = compose_embeddings_reactive(target, context)\n",
    "# dims = res.nonzero()[1]\n",
    "# explain_dims(dims, emb_nnse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Words] apple, delicious, ate, eat\n",
      "[('d868', 'peach, pear, raspberry, plum, mango', '0.76207'),\n",
      " ('d2095', 'oreo, crumb, kreme, krispy, shortbread', '0.60238'),\n",
      " ('d2203', 'godiva, starbucks, chocolate, candy, nestle', '0.21057'),\n",
      " ('d2239', 'rabe, raab, cheese, recipe, sauce', '0.10977')]\n"
     ]
    }
   ],
   "source": [
    "context = get_embeddings_for_words(['eat', 'ate', 'delicious'], emb_nnse)\n",
    "\n",
    "res = compose_embeddings_reactive(target, context)\n",
    "# dims = res.nonzero()[1]\n",
    "# explain_dims(dims, emb_nnse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_dataset`\n",
    "![wsd-dataset](./images/wsd-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"테스트 할 데이터셋을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 데이터셋 경로\n",
    "    Returns:\n",
    "        pandas.DataFrame: 로드한 데이터셋 DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexeme</th>\n",
       "      <th>Target Sense Definition</th>\n",
       "      <th>Target Sense Sentence</th>\n",
       "      <th>Example Definition Sense 1</th>\n",
       "      <th>Example Sentence Sense 1</th>\n",
       "      <th>Example Definition Sense 2</th>\n",
       "      <th>Example Sentence Sense 2</th>\n",
       "      <th>Data Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>Uncertainty prevails in the shadows of the Ira...</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>We've lived in the shadow of the seven-inch si...</td>\n",
       "      <td>a weak or inferior remnant or version of somet...</td>\n",
       "      <td>She had realized then, as she realized now, th...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bond</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Each union must buy an insurance bond to prote...</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Insurance company capital-protected guaranteed...</td>\n",
       "      <td>an agreement with legal force, in particular:</td>\n",
       "      <td>In this case, the defendant, a dyer, had given...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>We cannot rule out the possibility of a conspi...</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>Pat plans to release a further single in the n...</td>\n",
       "      <td>contracts for assets (especially commodities o...</td>\n",
       "      <td>Then as soon as the cash market closed, the S&amp;...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>So the true question of objective knowledge is...</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>As a rationalist, he believed that the only pa...</td>\n",
       "      <td>the sum of what is known</td>\n",
       "      <td>He does experimental and anthropological resea...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>If the show can be taken as a barometer of vis...</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>A lot of people who know nothing about art say...</td>\n",
       "      <td>works produced by human creative skill and ima...</td>\n",
       "      <td>The most economical way to sum it all up is wi...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lexeme                            Target Sense Definition  \\\n",
       "0     shadow  used in reference to proximity, ominous oppres...   \n",
       "1       bond  an insurance policy held by a company, which p...   \n",
       "2     future  a period of time following the moment of speak...   \n",
       "3  knowledge  true, justified belief; certain understanding,...   \n",
       "4        art  the expression or application of human creativ...   \n",
       "\n",
       "                               Target Sense Sentence  \\\n",
       "0  Uncertainty prevails in the shadows of the Ira...   \n",
       "1  Each union must buy an insurance bond to prote...   \n",
       "2  We cannot rule out the possibility of a conspi...   \n",
       "3  So the true question of objective knowledge is...   \n",
       "4  If the show can be taken as a barometer of vis...   \n",
       "\n",
       "                          Example Definition Sense 1  \\\n",
       "0  used in reference to proximity, ominous oppres...   \n",
       "1  an insurance policy held by a company, which p...   \n",
       "2  a period of time following the moment of speak...   \n",
       "3  true, justified belief; certain understanding,...   \n",
       "4  the expression or application of human creativ...   \n",
       "\n",
       "                            Example Sentence Sense 1  \\\n",
       "0  We've lived in the shadow of the seven-inch si...   \n",
       "1  Insurance company capital-protected guaranteed...   \n",
       "2  Pat plans to release a further single in the n...   \n",
       "3  As a rationalist, he believed that the only pa...   \n",
       "4  A lot of people who know nothing about art say...   \n",
       "\n",
       "                          Example Definition Sense 2  \\\n",
       "0  a weak or inferior remnant or version of somet...   \n",
       "1      an agreement with legal force, in particular:   \n",
       "2  contracts for assets (especially commodities o...   \n",
       "3                           the sum of what is known   \n",
       "4  works produced by human creative skill and ima...   \n",
       "\n",
       "                            Example Sentence Sense 2 Data Source  \n",
       "0  She had realized then, as she realized now, th...      Oxford  \n",
       "1  In this case, the defendant, a dyer, had given...      Oxford  \n",
       "2  Then as soon as the cash market closed, the S&...      Oxford  \n",
       "3  He does experimental and anthropological resea...      Oxford  \n",
       "4  The most economical way to sum it all up is wi...      Oxford  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드 예시\n",
    "f = './data/sense-2017/all/2_senses_dev_Noun.csv'\n",
    "dataset = load_dataset(f)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 로드\n",
    "emb_word2vec = load_word2vec()\n",
    "emb_nnse = load_nnse()\n",
    "\n",
    "# spaCy 모델(문맥 단어 추출에 사용) 로드\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_target_word_pos(test_case):\n",
    "#     \"\"\"테스트 케이스 명에서 Target word의 품사가 무엇인지 판단\n",
    "    \n",
    "#     제안하는 방법은 (명사, 동사, 형용사) 중 target word의 품사를 제외한 나머지 품사만을 문맥 단어로 활용함. \n",
    "    \n",
    "#     Args:\n",
    "#         test_case (str): 테스트 케이스 이름 (e.g. \"2_senses_dev_Adjective\")\n",
    "#     Returns:\n",
    "#         str: 'NOUN', 'VERB', 'ADJ' 중 하나 (spacy의 태그 형식)\n",
    "#     \"\"\"\n",
    "#     if 'Noun' in test_case:\n",
    "#         return 'NOUN'\n",
    "#     elif 'Verb' in test_case:\n",
    "#         return 'VERB'\n",
    "#     elif 'Adjective' in test_case:\n",
    "#         return 'ADJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2_senses_dev_Noun]\n",
      "sum_word2vec: \t0.7278 (123 / 169)\n",
      "sum_nnse: \t0.5503 (93 / 169)\n",
      "reactive_nnse: \t0.3018 (51 / 169)\n",
      "\n",
      "[3_senses_dev_Noun]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-b98b0cbee194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mcontext_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m# spaCy Universal Part-of-speech Tags (https://spacy.io/api/annotation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             contexts = [[cxt for cxt in extract_context_words(sent, nlp, context_pos) if cxt != target_word] # target word는 제거\n\u001b[1;32m---> 54\u001b[1;33m                             for sent in sentences]\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nnse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embeddings_for_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_nnse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nnse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contexts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_embeddings_for_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_nnse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-b98b0cbee194>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mcontext_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m# spaCy Universal Part-of-speech Tags (https://spacy.io/api/annotation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             contexts = [[cxt for cxt in extract_context_words(sent, nlp, context_pos) if cxt != target_word] # target word는 제거\n\u001b[1;32m---> 54\u001b[1;33m                             for sent in sentences]\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nnse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embeddings_for_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_nnse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nnse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contexts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_embeddings_for_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_nnse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-711e16249bec>\u001b[0m in \u001b[0;36mextract_context_words\u001b[1;34m(text, nlp, context_pos)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m텍스트에서\u001b[0m \u001b[0m추출한\u001b[0m \u001b[0m문맥\u001b[0m \u001b[0m단어\u001b[0m \u001b[0m리스트\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'단어 소문자'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0m형태\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mstopword\u001b[0m \u001b[0m제외\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#     return [(tok.text.lower(), tok.pos_) for tok in doc if tok.pos_ in ('NOUN', 'VERB')]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext_pos\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    # 데이터셋 파일 목록 읽어오기\n",
    "    files = glob('./data/sense-2017/all/*.csv')\n",
    "    files = [f for f in files if\n",
    "#                  '3_senses' in f and\n",
    "                 'dev' in f and\n",
    "                 'Noun' in f]\n",
    "#     files = files[:1]\n",
    "    \n",
    "    # 각 데이터셋 별로\n",
    "    corrects_all = {}\n",
    "    debugs_all = {}\n",
    "    for f in files:\n",
    "        test_case = os.path.basename(f).split('.')[0]\n",
    "        print('[{}]'.format(test_case))\n",
    "\n",
    "        # 데이터셋 로드\n",
    "        dataset = load_dataset(f)\n",
    "\n",
    "        # 1. 데이터셋 각 row 별로,\n",
    "        corrects_testcase = defaultdict(list) # row 별로 점답 여부 저장 \n",
    "        debugs_testcase = defaultdict(list)   # row 별로 디버그 정보 저장 (합성 벡터, Option 별 점수 등)\n",
    "        for i, row in dataset.iterrows():\n",
    "            # 2. Target 단어 및 Sentence(Target, Option #) 추출\n",
    "            #     sentences[0] = target-sentence, sentences[1:] = option-sentences\n",
    "            target_word = row['Lexeme'].strip()                \n",
    "            sentences = [row[col].strip() for col in row.keys() if 'Sentence' in col]\n",
    "            if DEBUG:\n",
    "                print('<{}>'.format(target_word))\n",
    "\n",
    "            # 3. Target 단어 및 context 단어들의 임베딩 확보 (`emb_word2vec`, `emb_nnse`)\n",
    "            embeddings = {\n",
    "                'word2vec': {\n",
    "                    'target': None,\n",
    "                    'contexts': [None, None]\n",
    "                },\n",
    "                'nnse': {\n",
    "                    'target': None,\n",
    "                    'contexts': [None, None]\n",
    "                }\n",
    "            }\n",
    "            ## Context 추출\n",
    "            context_pos = {'NOUN', 'VERB', 'ADJ'} # spaCy Universal Part-of-speech Tags (https://spacy.io/api/annotation)\n",
    "            contexts = [[cxt for cxt in extract_context_words(sent, nlp, context_pos) if cxt != target_word] # target word는 제거\n",
    "                            for sent in sentences]\n",
    "            \n",
    "            ## word2vec 임베딩 확보 (Target & Context)\n",
    "            embeddings['word2vec']['target'] = get_embeddings_for_words([target_word], emb_word2vec)\n",
    "            embeddings['word2vec']['contexts'] = [get_embeddings_for_words(cxt, emb_word2vec) for cxt in contexts]\n",
    "\n",
    "            ## NNSE 임베딩 확보 (Target & Context)\n",
    "            embeddings['nnse']['target'] = get_embeddings_for_words([target_word], emb_nnse)\n",
    "            embeddings['nnse']['contexts'] = [get_embeddings_for_words(cxt, emb_nnse) for cxt in contexts]\n",
    "\n",
    "            # 5. 임베딩 유형 & 합성 방식 별 성능 비교 (word2vec vs. NNSE & sum vs. reactive)\n",
    "            #     (target & contexts[0]) VS (target & contexts[1]), (target & contexts[2]), ...\n",
    "            #     여기서 contexts[0]는 \"Target Sentence\"에서 추출된 문맥 임베딩, 나머지는 \"Option Sentence\"\n",
    "            ## 합성 방식 별 비교\n",
    "            compose_methods = {\n",
    "                'sum': compose_embeddings_sum,\n",
    "                'reactive': compose_embeddings_reactive\n",
    "            }\n",
    "            for compose_type in ['sum', 'reactive']:\n",
    "                compose_method = compose_methods[compose_type]\n",
    "\n",
    "                ## 임베딩 유형별 비교\n",
    "                emb_types = ['word2vec', 'nnse'] if compose_type == 'sum' else ['nnse']\n",
    "                for emb_type in emb_types:\n",
    "                    # 주어진 유형의 임베딩 확보\n",
    "                    emb_sets = embeddings[emb_type]\n",
    "\n",
    "                    # 실험 프로세스\n",
    "                    setting = '{}_{}'.format(compose_type, emb_type)  # 현재 설정 (포멧: \"합성방법_임베딩유형\")\n",
    "\n",
    "                    if emb_sets['target'].size == 0:  # Target 단어가 임베딩 행렬에 없을 경우 생략\n",
    "                        continue\n",
    "                    else:                             # Target 단어가 있을 경우\n",
    "                        # Target 합성 임베딩 계산\n",
    "                        target_composed = compose_method(emb_sets['target'], emb_sets['contexts'][0])\n",
    "                        # Option 합성 임베딩 계산\n",
    "                        options_composed = [compose_method(emb_sets['target'], cxt) for cxt in emb_sets['contexts'][1:]]\n",
    "\n",
    "                        # Target과 Option 간 cosine similarity 점수 계산 & Option 별 점수 랭킹 계산\n",
    "                        # Option 1이 항상 정답임\n",
    "                        try:\n",
    "                            option_scores = [cosine_similarity(target_composed, opt)\n",
    "                                         for opt in options_composed]\n",
    "                            # 정답 여부 확인\n",
    "                            ## score 계산이 잘못되어 모든 option의 점수가 같을 경우에도\n",
    "                            ## 맨 앞 index인 0이 출력되어 맞다고 잘못 판정되는 것 방지\n",
    "                            last_option_idx = len(option_scores)-1 \n",
    "                            option_scores_rev = np.flip(option_scores, axis=0)\n",
    "                            ## 정답 여부 체크\n",
    "                            correct = ( np.argmax(option_scores_rev) == last_option_idx )\n",
    "                        except ValueError: # NNSE의 경우 context 단어 벡터를 하나도 찾지 못하는 경우가 생김\n",
    "                            # 틀렸다고 간주\n",
    "                            correct = False\n",
    "                        # 결과 저장 (row)\n",
    "                        ## 정답 여부 저장\n",
    "                        corrects_testcase[setting].append(correct)\n",
    "                        ## 디버그 정보 저장\n",
    "                        debug = {\n",
    "                            'target_composed': target_composed,\n",
    "                            'options_composed': options_composed,\n",
    "                            'option_scores': option_scores\n",
    "                        }\n",
    "                        debugs_testcase[setting].append(debug)  \n",
    "\n",
    "            # 결과 저장 (test case)\n",
    "            ## 정답 여부 저장\n",
    "            corrects_all[test_case] = corrects_testcase\n",
    "            ## 디버그 정보 저장\n",
    "            debugs_all[test_case] = debugs_testcase\n",
    "            \n",
    "            # TODO: 테스트 용. 나중에 제거.\n",
    "#             if i > 3:\n",
    "#                 break\n",
    "            \n",
    "        # 점수 출력 (per testcase)\n",
    "        for setting, corrects in corrects_testcase.items():\n",
    "            n_correct = sum(corrects)\n",
    "            n_all = len(corrects)\n",
    "            final_score = n_correct / n_all\n",
    "            print('{}: \\t{:.4f} ({} / {})'.format(setting, final_score, n_correct, n_all))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# debugs_all['2_senses_dev_Verb']['reactive_nnse']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
