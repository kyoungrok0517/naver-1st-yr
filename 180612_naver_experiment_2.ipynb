{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `extract_context_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_words(text, nlp):\n",
    "    \"\"\"입력된 text로부터 명사/동사 추출\n",
    "    \n",
    "    Args:\n",
    "        text (str): Context 추출할 텍스트\n",
    "        nlp (spacy.lang.en.English): spaCy 모델\n",
    "    Returns:\n",
    "        list: 텍스트에서 추출한 명사/동사 리스트. ['단어 소문자'] 형태. stopword 제외\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "#     return [(tok.text.lower(), tok.pos_) for tok in doc if tok.pos_ in ('NOUN', 'VERB')]\n",
    "    return [tok.text.lower() for tok in doc if tok.pos_ in ('NOUN', 'VERB') and not tok.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization',\n",
       " 'standards',\n",
       " 'based',\n",
       " 'corpus',\n",
       " 'tokenizer',\n",
       " 'differs',\n",
       " 'including',\n",
       " 'tokens',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Tokenization standards are based on the OntoNotes 5 corpus. The tokenizer differs from most by including tokens for significant whitespace.'\n",
    "nlp = spacy.load('en')\n",
    "extract_context_words(text, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_word2vec`, `load_nnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec():\n",
    "    \"\"\"word2vec 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('../data/embeddings/word2vec_300.parquet')\n",
    "    return embeddings\n",
    "\n",
    "def load_nnse():\n",
    "    \"\"\"NNSE 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('../data/embeddings/nnse_2500.parquet')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d291</th>\n",
       "      <th>d292</th>\n",
       "      <th>d293</th>\n",
       "      <th>d294</th>\n",
       "      <th>d295</th>\n",
       "      <th>d296</th>\n",
       "      <th>d297</th>\n",
       "      <th>d298</th>\n",
       "      <th>d299</th>\n",
       "      <th>d300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.050293</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.026733</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.070801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            d1        d2        d3        d4        d5        d6        d7  \\\n",
       "word                                                                         \n",
       "in    0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "for  -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "that -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
       "is    0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "on    0.026733 -0.090820  0.027832  0.204102  0.006226 -0.090332  0.022583   \n",
       "\n",
       "            d8        d9       d10    ...         d291      d292      d293  \\\n",
       "word                                  ...                                    \n",
       "in   -0.154297  0.020752  0.131836    ...    -0.168945 -0.088867 -0.080566   \n",
       "for  -0.072266  0.064453  0.086426    ...    -0.022583  0.003723 -0.082520   \n",
       "that -0.089844  0.117676  0.118164    ...    -0.011292 -0.015625 -0.033447   \n",
       "is   -0.107910  0.071777  0.020874    ...    -0.233398 -0.036377 -0.093750   \n",
       "on   -0.161133  0.132812  0.061035    ...     0.026855 -0.027954  0.030884   \n",
       "\n",
       "          d294      d295      d296      d297      d298      d299      d300  \n",
       "word                                                                        \n",
       "in    0.064941  0.061279 -0.047363 -0.058838 -0.047607  0.014465 -0.062500  \n",
       "for   0.081543  0.007935  0.000477  0.018433  0.071289 -0.034912  0.024170  \n",
       "that -0.020630 -0.019409  0.063965  0.020142  0.006866  0.061035 -0.148438  \n",
       "is    0.182617  0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934  \n",
       "on    0.040527 -0.130859  0.083008  0.015747 -0.116699 -0.029419 -0.070801  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = load_word2vec()\n",
    "emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'for', 'that', 'is', 'on']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.head().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_embeddings_for_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_words(words, embeddings):\n",
    "    \"\"\"주어진 단어들의 임베딩을 `pandas.DataFrame` 형태로 반환\n",
    "    \n",
    "    Args:\n",
    "        words (list): 단어 리스트\n",
    "        embeddings (pandas.DataFrame): 임베딩 행렬 (`load_word2vec`, `load_nnse` 반환 형태)\n",
    "    Returns:\n",
    "        pandas.DataFrame: 주어진 단어들의 임베딩\n",
    "        \n",
    "        단어가 임베딩 행렬에 없는 경우 제외\n",
    "    \"\"\"\n",
    "    return embeddings.loc[embeddings.index.intersection(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d291</th>\n",
       "      <th>d292</th>\n",
       "      <th>d293</th>\n",
       "      <th>d294</th>\n",
       "      <th>d295</th>\n",
       "      <th>d296</th>\n",
       "      <th>d297</th>\n",
       "      <th>d298</th>\n",
       "      <th>d299</th>\n",
       "      <th>d300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>-0.060547</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.024048</td>\n",
       "      <td>-0.017090</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>-0.038086</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003387</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>0.126953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standards</th>\n",
       "      <td>-0.251953</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.392578</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.027588</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.012207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>differs</th>\n",
       "      <td>0.017212</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>0.049561</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>-0.037598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049805</td>\n",
       "      <td>-0.121094</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>-0.285156</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.384766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 d1        d2        d3        d4        d5        d6  \\\n",
       "including -0.060547  0.073242  0.024048 -0.017090  0.108398  0.002121   \n",
       "standards -0.251953  0.124023  0.392578  0.283203 -0.093750  0.065918   \n",
       "differs    0.017212 -0.082520  0.074707 -0.001503 -0.069336  0.330078   \n",
       "\n",
       "                 d7        d8        d9       d10    ...         d291  \\\n",
       "including -0.038086  0.038818  0.078613  0.074707    ...    -0.003387   \n",
       "standards  0.027588 -0.063477  0.086426  0.045166    ...     0.046143   \n",
       "differs    0.121094  0.049561  0.201172 -0.037598    ...    -0.049805   \n",
       "\n",
       "               d292      d293      d294      d295      d296      d297  \\\n",
       "including -0.035156  0.052979  0.030029 -0.019531 -0.114746  0.102051   \n",
       "standards  0.004761 -0.273438  0.079590  0.085449  0.053223  0.168945   \n",
       "differs   -0.121094 -0.025513  0.174805 -0.104492  0.122559  0.162109   \n",
       "\n",
       "               d298      d299      d300  \n",
       "including  0.022339  0.012268  0.126953  \n",
       "standards -0.164062  0.057861  0.012207  \n",
       "differs   -0.285156 -0.090820  0.384766  \n",
       "\n",
       "[3 rows x 300 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['standards', 'differs', 'including', 'abcc'] # 'abcc'는 행렬에 없음\n",
    "word_embeddings = get_embeddings_for_words(words, emb)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_sum` (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_embeddings_sum(target_embs, context_embs):\n",
    "    \"\"\"[Baseline] 주어진 단어 임베딩들의 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    단순 덧셈\n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (덧셈). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    embs = pd.concat([target_embs, context_embs])\n",
    "    return embs.sum(axis=0).values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compose_embeddings_sum(word_embeddings.iloc[1:], word_embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_reactive` (My)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    \"\"\"\n",
    "    Calculate the softmax for the give inputs (array)\n",
    "    :param inputs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.exp(inputs) / float(sum(np.exp(inputs)))\n",
    "\n",
    "def compose_embeddings_reactive(target_embs, context_embs):\n",
    "    \"\"\"[Proposing] 주어진 단어 임베딩들의 contextualized 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    1. \n",
    "    2. \n",
    "    3. \n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (Contextualized). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    #  target에서 0이 아닌 차원 파악\n",
    "    target_nonzero = target_embs.values.nonzero()\n",
    "    \n",
    "    # context에서 0이 아닌 차원을 값이 큰 순으로 정렬한 후, 상위 top_dim_k 만큼만 파악.\n",
    "    # top_dim_k은 차원_갯수*0.01 (예: 3000*0.01 = 30)\n",
    "    top_dim_k = int(len(target_embs) * 0.01)\n",
    "    context = context_embs.sum()\n",
    "    context_top_nonzero = np.flip(np.argsort(context.values), axis=0)[:top_dim_k]\n",
    "    \n",
    "    # both_nonzero_dims : target_nonzero, context_top_nonzero의 교집합\n",
    "    both_nonzero_dims = np.intersect1d(target_nonzero, context_top_nonzero)\n",
    "    \n",
    "    # target_embs의 나머지 차원 비활성화\n",
    "    final_emb = target_embs.values\n",
    "    for d in range(0, len(final_emb)):\n",
    "        if d not in both_nonzero_dims:\n",
    "#             print('Setting {} dim = 0.0'.format(d))\n",
    "            final_emb[d] = 0.0\n",
    "    \n",
    "    # 다음 추가 처리 후 반환\n",
    "    #   1. 정규화 (softmax)\n",
    "    #   2. (1, #_of_dimensions) 형태로 reshape\n",
    "    final_emb = softmax(final_emb.reshape(-1, 1))\n",
    "    return final_emb.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `explain_dim`, `explain_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_dim(dim, embeddings, k=5):\n",
    "    return embeddings.sort_values(by=dim, ascending=False).index.tolist()[:k]\n",
    "\n",
    "def explain_dims(dims, embeddings, k=5):\n",
    "    return [', '.join(explain_dim(d, embeddings, k=k)) for d in dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['girls', 'boys', 'powerpuff', 'backstreet', 'gilmore']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_dim('d2', emb_nnse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_dataset`\n",
    "![wsd-dataset](./images/wsd-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"테스트 할 데이터셋을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 데이터셋 경로\n",
    "    Returns:\n",
    "        pandas.DataFrame: 로드한 데이터셋 DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexeme</th>\n",
       "      <th>Target Sense Definition</th>\n",
       "      <th>Target Sense Sentence</th>\n",
       "      <th>Example Definition Sense 1</th>\n",
       "      <th>Example Sentence Sense 1</th>\n",
       "      <th>Example Definition Sense 2</th>\n",
       "      <th>Example Sentence Sense 2</th>\n",
       "      <th>Data Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>Uncertainty prevails in the shadows of the Ira...</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>We've lived in the shadow of the seven-inch si...</td>\n",
       "      <td>a weak or inferior remnant or version of somet...</td>\n",
       "      <td>She had realized then, as she realized now, th...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bond</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Each union must buy an insurance bond to prote...</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Insurance company capital-protected guaranteed...</td>\n",
       "      <td>an agreement with legal force, in particular:</td>\n",
       "      <td>In this case, the defendant, a dyer, had given...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>We cannot rule out the possibility of a conspi...</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>Pat plans to release a further single in the n...</td>\n",
       "      <td>contracts for assets (especially commodities o...</td>\n",
       "      <td>Then as soon as the cash market closed, the S&amp;...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>So the true question of objective knowledge is...</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>As a rationalist, he believed that the only pa...</td>\n",
       "      <td>the sum of what is known</td>\n",
       "      <td>He does experimental and anthropological resea...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>If the show can be taken as a barometer of vis...</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>A lot of people who know nothing about art say...</td>\n",
       "      <td>works produced by human creative skill and ima...</td>\n",
       "      <td>The most economical way to sum it all up is wi...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lexeme                            Target Sense Definition  \\\n",
       "0     shadow  used in reference to proximity, ominous oppres...   \n",
       "1       bond  an insurance policy held by a company, which p...   \n",
       "2     future  a period of time following the moment of speak...   \n",
       "3  knowledge  true, justified belief; certain understanding,...   \n",
       "4        art  the expression or application of human creativ...   \n",
       "\n",
       "                               Target Sense Sentence  \\\n",
       "0  Uncertainty prevails in the shadows of the Ira...   \n",
       "1  Each union must buy an insurance bond to prote...   \n",
       "2  We cannot rule out the possibility of a conspi...   \n",
       "3  So the true question of objective knowledge is...   \n",
       "4  If the show can be taken as a barometer of vis...   \n",
       "\n",
       "                          Example Definition Sense 1  \\\n",
       "0  used in reference to proximity, ominous oppres...   \n",
       "1  an insurance policy held by a company, which p...   \n",
       "2  a period of time following the moment of speak...   \n",
       "3  true, justified belief; certain understanding,...   \n",
       "4  the expression or application of human creativ...   \n",
       "\n",
       "                            Example Sentence Sense 1  \\\n",
       "0  We've lived in the shadow of the seven-inch si...   \n",
       "1  Insurance company capital-protected guaranteed...   \n",
       "2  Pat plans to release a further single in the n...   \n",
       "3  As a rationalist, he believed that the only pa...   \n",
       "4  A lot of people who know nothing about art say...   \n",
       "\n",
       "                          Example Definition Sense 2  \\\n",
       "0  a weak or inferior remnant or version of somet...   \n",
       "1      an agreement with legal force, in particular:   \n",
       "2  contracts for assets (especially commodities o...   \n",
       "3                           the sum of what is known   \n",
       "4  works produced by human creative skill and ima...   \n",
       "\n",
       "                            Example Sentence Sense 2 Data Source  \n",
       "0  She had realized then, as she realized now, th...      Oxford  \n",
       "1  In this case, the defendant, a dyer, had given...      Oxford  \n",
       "2  Then as soon as the cash market closed, the S&...      Oxford  \n",
       "3  He does experimental and anthropological resea...      Oxford  \n",
       "4  The most economical way to sum it all up is wi...      Oxford  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = '../data/sense-2017/all/2_senses_dev_Noun.csv'\n",
    "dataset = load_dataset(f)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 로드\n",
    "emb_word2vec = load_word2vec()\n",
    "emb_nnse = load_nnse()\n",
    "\n",
    "# spaCy 모델(문맥 단어 추출에 사용) 로드\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2_senses_dev_Adjective]\n",
      "sum_word2vec: \t0.6970 (46 / 66)\n",
      "sum_nnse: \t0.6212 (41 / 66)\n",
      "reactive_nnse: \t1.0000 (66 / 66)\n",
      "\n",
      "[5_senses_test_Adjective]\n",
      "sum_word2vec: \t0.6087 (70 / 115)\n",
      "sum_nnse: \t0.5217 (60 / 115)\n",
      "reactive_nnse: \t1.0000 (115 / 115)\n",
      "\n",
      "[2_senses_test_Adjective]\n",
      "sum_word2vec: \t0.7129 (149 / 209)\n",
      "sum_nnse: \t0.7033 (147 / 209)\n",
      "reactive_nnse: \t1.0000 (209 / 209)\n",
      "\n",
      "[2_senses_test_Noun]\n",
      "sum_word2vec: \t0.7395 (457 / 618)\n",
      "sum_nnse: \t0.6796 (420 / 618)\n",
      "reactive_nnse: \t0.9984 (617 / 618)\n",
      "\n",
      "[3_senses_test_Verb]\n",
      "sum_word2vec: \t0.6384 (226 / 354)\n",
      "sum_nnse: \t0.5367 (190 / 354)\n",
      "reactive_nnse: \t0.9718 (344 / 354)\n",
      "\n",
      "[3_senses_dev_Adjective]\n",
      "sum_word2vec: \t0.5745 (27 / 47)\n",
      "sum_nnse: \t0.5532 (26 / 47)\n",
      "reactive_nnse: \t1.0000 (47 / 47)\n",
      "\n",
      "[2_senses_test_Verb]\n",
      "sum_word2vec: \t0.7078 (310 / 438)\n",
      "sum_nnse: \t0.6689 (293 / 438)\n",
      "reactive_nnse: \t0.9749 (427 / 438)\n",
      "\n",
      "[3_senses_test_Noun]\n",
      "sum_word2vec: \t0.6012 (300 / 499)\n",
      "sum_nnse: \t0.5170 (258 / 499)\n",
      "reactive_nnse: \t0.9960 (497 / 499)\n",
      "\n",
      "[5_senses_test_Noun]\n",
      "sum_word2vec: \t0.5536 (191 / 345)\n",
      "sum_nnse: \t0.3942 (136 / 345)\n",
      "reactive_nnse: \t1.0000 (345 / 345)\n",
      "\n",
      "[3_senses_dev_Noun]\n",
      "sum_word2vec: \t0.5280 (66 / 125)\n",
      "sum_nnse: \t0.5040 (63 / 125)\n",
      "reactive_nnse: \t1.0000 (125 / 125)\n",
      "\n",
      "[4_senses_test_Verb]\n",
      "sum_word2vec: \t0.5492 (162 / 295)\n",
      "sum_nnse: \t0.4644 (137 / 295)\n",
      "reactive_nnse: \t0.9864 (291 / 295)\n",
      "\n",
      "[2_senses_dev_Noun]\n",
      "sum_word2vec: \t0.7353 (125 / 170)\n",
      "sum_nnse: \t0.6118 (104 / 170)\n",
      "reactive_nnse: \t0.9941 (169 / 170)\n",
      "\n",
      "[4_senses_dev_Noun]\n",
      "sum_word2vec: \t0.5300 (53 / 100)\n",
      "sum_nnse: \t0.4200 (42 / 100)\n",
      "reactive_nnse: \t1.0000 (100 / 100)\n",
      "\n",
      "[5_senses_dev_Noun]\n",
      "sum_word2vec: \t0.5811 (43 / 74)\n",
      "sum_nnse: \t0.3649 (27 / 74)\n",
      "reactive_nnse: \t0.9865 (73 / 74)\n",
      "\n",
      "[5_senses_dev_Adjective]\n",
      "sum_word2vec: \t0.7143 (20 / 28)\n",
      "sum_nnse: \t0.6071 (17 / 28)\n",
      "reactive_nnse: \t1.0000 (28 / 28)\n",
      "\n",
      "[5_senses_test_Verb]\n",
      "sum_word2vec: \t0.5117 (131 / 256)\n",
      "sum_nnse: \t0.3906 (100 / 256)\n",
      "reactive_nnse: \t0.9805 (251 / 256)\n",
      "\n",
      "[4_senses_test_Noun]\n",
      "sum_word2vec: \t0.5777 (238 / 412)\n",
      "sum_nnse: \t0.4563 (188 / 412)\n",
      "reactive_nnse: \t0.9976 (411 / 412)\n",
      "\n",
      "[2_senses_dev_Verb]\n",
      "sum_word2vec: \t0.7638 (97 / 127)\n",
      "sum_nnse: \t0.6614 (84 / 127)\n",
      "reactive_nnse: \t0.9843 (125 / 127)\n",
      "\n",
      "[3_senses_test_Adjective]\n",
      "sum_word2vec: \t0.6353 (108 / 170)\n",
      "sum_nnse: \t0.5941 (101 / 170)\n",
      "reactive_nnse: \t1.0000 (170 / 170)\n",
      "\n",
      "[4_senses_dev_Adjective]\n",
      "sum_word2vec: \t0.5676 (21 / 37)\n",
      "sum_nnse: \t0.3784 (14 / 37)\n",
      "reactive_nnse: \t1.0000 (37 / 37)\n",
      "\n",
      "[3_senses_dev_Verb]\n",
      "sum_word2vec: \t0.5385 (49 / 91)\n",
      "sum_nnse: \t0.5385 (49 / 91)\n",
      "reactive_nnse: \t0.9890 (90 / 91)\n",
      "\n",
      "[5_senses_dev_Verb]\n",
      "sum_word2vec: \t0.5926 (32 / 54)\n",
      "sum_nnse: \t0.5370 (29 / 54)\n",
      "reactive_nnse: \t1.0000 (54 / 54)\n",
      "\n",
      "[4_senses_test_Adjective]\n",
      "sum_word2vec: \t0.6569 (90 / 137)\n",
      "sum_nnse: \t0.6131 (84 / 137)\n",
      "reactive_nnse: \t1.0000 (137 / 137)\n",
      "\n",
      "[4_senses_dev_Verb]\n",
      "sum_word2vec: \t0.5833 (42 / 72)\n",
      "sum_nnse: \t0.5417 (39 / 72)\n",
      "reactive_nnse: \t0.9861 (71 / 72)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    # 데이터셋 파일 목록 읽어오기\n",
    "    files = glob('../data/sense-2017/all/*.csv')\n",
    "#     files = [f for f in files if 'Noun' in f]\n",
    "#     files = files[5:7]\n",
    "    \n",
    "    # 각 데이터셋 별로 실험 수행\n",
    "    for f in files:\n",
    "        test_case = os.path.basename(f).split('.')[0]\n",
    "        print('[{}]'.format(test_case))\n",
    "        \n",
    "        # 데이터셋 DataFrame 로드\n",
    "        dataset = load_dataset(f)\n",
    "        \n",
    "        # 1. 데이터셋 각 행(case) 별로,\n",
    "        corrects_all = defaultdict(list) # row 별로 True, False 여부 저장 (계산은 for-loop 마지막에)\n",
    "        for _, row in dataset.iterrows():\n",
    "            # 2. Target 단어 및 Sentence(Target, Option #) 추출\n",
    "            #     sentences[0] = target-sentence, sentences[1:] = option-sentences\n",
    "            target_word = row['Lexeme']\n",
    "            sentences = [row[col] for col in row.keys() if 'Sentence' in col]\n",
    "            \n",
    "            # 3. sentences에서 context 단어(명사, 동사) 추출\n",
    "            contexts = [extract_context_words(sent, nlp) for sent in sentences]\n",
    "            \n",
    "            # 4. Target 단어 및 context 단어들의 임베딩 확보 (`emb_word2vec`, `emb_nnse`)\n",
    "            embeddings = {\n",
    "                'word2vec': {\n",
    "                    'target': None,\n",
    "                    'contexts': [None, None]\n",
    "                },\n",
    "                'nnse': {\n",
    "                    'target': None,\n",
    "                    'contexts': [None, None]\n",
    "                }\n",
    "            }\n",
    "            ## word2vec 임베딩 확보\n",
    "            emb = emb_word2vec\n",
    "            embeddings['word2vec']['target'] = get_embeddings_for_words([target_word], emb)\n",
    "            embeddings['word2vec']['contexts'] = [get_embeddings_for_words(cxt, emb) for cxt in contexts]\n",
    "            \n",
    "            ## NNSE 임베딩 확보\n",
    "            emb = emb_nnse\n",
    "            embeddings['nnse']['target'] = get_embeddings_for_words([target_word], emb)\n",
    "            embeddings['nnse']['contexts'] = [get_embeddings_for_words(cxt, emb) for cxt in contexts]\n",
    "            \n",
    "            # 5. 임베딩 유형 & 합성 방식 별 성능 비교 (word2vec vs. NNSE & sum vs. reactive)\n",
    "            #     (target & contexts[0]) VS (target & contexts[1]), (target & contexts[2]), ...\n",
    "            #     여기서 contexts[0]는 \"Target Sentence\"에서 추출된 문맥 임베딩, 나머지는 \"Option Sentence\"\n",
    "            \n",
    "            ## 합성 방식 별 비교\n",
    "            compose_methods = {\n",
    "                'sum': compose_embeddings_sum,\n",
    "                'reactive': compose_embeddings_reactive\n",
    "            }\n",
    "            for compose_type in ['sum', 'reactive']:\n",
    "                compose_method = compose_methods[compose_type]\n",
    "                \n",
    "                ## 임베딩 유형별 비교\n",
    "                emb_types = ['word2vec', 'nnse'] if compose_type == 'sum' else ['nnse']\n",
    "                for emb_type in emb_types:\n",
    "                    # 주어진 유형의 임베딩 확보\n",
    "                    emb_sets = embeddings[emb_type]\n",
    "\n",
    "                    # Target 합성 임베딩 계산\n",
    "                    target_composed = compose_method(emb_sets['target'], emb_sets['contexts'][0])\n",
    "                    # Option 합성 임베딩 계산\n",
    "                    options_composed = [compose_method(emb_sets['target'], cxt) for cxt in emb_sets['contexts'][1:]]\n",
    "\n",
    "                    # Target과 Option 간 cosine similarity 점수 계산 & Option 별 점수 랭킹 계산\n",
    "                    # Option 1이 항상 정답임\n",
    "                    setting = '{}_{}'.format(compose_type, emb_type)\n",
    "                    try:\n",
    "                        option_scores = [cosine_similarity(target_composed, opt)\n",
    "                                     for opt in options_composed]\n",
    "                        correct = (np.argmax(option_scores) == 0)\n",
    "                    \n",
    "                        # Save result\n",
    "                        corrects_all[setting].append(correct)\n",
    "                    except ValueError: # NNSE의 경우 context 단어 벡터를 하나도 찾지 못하는 경우가 생김\n",
    "#                         print('Error:', target_word, setting)\n",
    "                        corrects_all[setting].append(False) # 틀렸다고 간주\n",
    "                    \n",
    "                \n",
    "        # 점수 출력\n",
    "        for setting, corrects in corrects_all.items():\n",
    "            n_correct = sum(corrects)\n",
    "            n_all = len(corrects)\n",
    "            final_score = n_correct / n_all\n",
    "            print('{}: \\t{:.4f} ({} / {})'.format(setting, final_score, n_correct, n_all))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
