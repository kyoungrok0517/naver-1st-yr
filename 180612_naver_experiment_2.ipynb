{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `extract_context_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_words(text, nlp, context_pos=('NOUN', 'VERB', 'ADJ')):\n",
    "    \"\"\"입력된 text로부터 명사/동사 추출\n",
    "    \n",
    "    Args:\n",
    "        text (str): Context 추출할 텍스트\n",
    "        nlp (spacy.lang.en.English): spaCy 모델\n",
    "        context_pos (list): 추출할 품사\n",
    "    Returns:\n",
    "        list: 텍스트에서 추출한 문맥 단어 리스트. ['단어 소문자'] 형태. stopword 제외\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "#     return [(tok.text.lower(), tok.pos_) for tok in doc if tok.pos_ in ('NOUN', 'VERB')]\n",
    "    return [tok.lemma_.lower() for tok in doc if tok.pos_ in context_pos and not tok.is_stop]\n",
    "#     return [tok.text.lower() for tok in doc if not tok.is_stop and tok.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenization',\n",
       " 'big',\n",
       " 'standard',\n",
       " 'base',\n",
       " 'corpus',\n",
       " 'tokenizer',\n",
       " 'differ',\n",
       " 'include',\n",
       " 'token',\n",
       " 'significant',\n",
       " 'whitespace']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Tokenization big standards are based on the OntoNotes 5 corpus. The tokenizer differs from most by including tokens for significant whitespace.'\n",
    "nlp = spacy.load('en')\n",
    "extract_context_words(text, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_word2vec`, `load_nnse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec():\n",
    "    \"\"\"word2vec 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('../data/embeddings/word2vec_300.parquet')\n",
    "    return embeddings\n",
    "\n",
    "def load_nnse():\n",
    "    \"\"\"NNSE 임베딩 행렬을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Index: 단어, Column: 차원 값 형태\n",
    "    \"\"\"\n",
    "    embeddings = pd.read_parquet('../data/embeddings/nnse_2500.parquet')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d291</th>\n",
       "      <th>d292</th>\n",
       "      <th>d293</th>\n",
       "      <th>d294</th>\n",
       "      <th>d295</th>\n",
       "      <th>d296</th>\n",
       "      <th>d297</th>\n",
       "      <th>d298</th>\n",
       "      <th>d299</th>\n",
       "      <th>d300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.024170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.050293</td>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.020630</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.026733</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.027954</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.029419</td>\n",
       "      <td>-0.070801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            d1        d2        d3        d4        d5        d6        d7  \\\n",
       "word                                                                         \n",
       "in    0.070312  0.086914  0.087891  0.062500  0.069336 -0.108887 -0.081543   \n",
       "for  -0.011780 -0.047363  0.044678  0.063477 -0.018188 -0.063965 -0.001312   \n",
       "that -0.015747 -0.028320  0.083496  0.050293 -0.110352  0.031738 -0.014221   \n",
       "is    0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "on    0.026733 -0.090820  0.027832  0.204102  0.006226 -0.090332  0.022583   \n",
       "\n",
       "            d8        d9       d10    ...         d291      d292      d293  \\\n",
       "word                                  ...                                    \n",
       "in   -0.154297  0.020752  0.131836    ...    -0.168945 -0.088867 -0.080566   \n",
       "for  -0.072266  0.064453  0.086426    ...    -0.022583  0.003723 -0.082520   \n",
       "that -0.089844  0.117676  0.118164    ...    -0.011292 -0.015625 -0.033447   \n",
       "is   -0.107910  0.071777  0.020874    ...    -0.233398 -0.036377 -0.093750   \n",
       "on   -0.161133  0.132812  0.061035    ...     0.026855 -0.027954  0.030884   \n",
       "\n",
       "          d294      d295      d296      d297      d298      d299      d300  \n",
       "word                                                                        \n",
       "in    0.064941  0.061279 -0.047363 -0.058838 -0.047607  0.014465 -0.062500  \n",
       "for   0.081543  0.007935  0.000477  0.018433  0.071289 -0.034912  0.024170  \n",
       "that -0.020630 -0.019409  0.063965  0.020142  0.006866  0.061035 -0.148438  \n",
       "is    0.182617  0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934  \n",
       "on    0.040527 -0.130859  0.083008  0.015747 -0.116699 -0.029419 -0.070801  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_word2vec = load_word2vec()\n",
    "emb_nnse = load_nnse()\n",
    "emb_word2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'for', 'that', 'is', 'on']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = 단어\n",
    "emb_word2vec.head().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_embeddings_for_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_words(words, embeddings):\n",
    "    \"\"\"주어진 단어들의 임베딩을 `pandas.DataFrame` 형태로 반환\n",
    "    \n",
    "    Args:\n",
    "        words (list): 단어 리스트\n",
    "        embeddings (pandas.DataFrame): 임베딩 행렬 (`load_word2vec`, `load_nnse` 반환 형태)\n",
    "    Returns:\n",
    "        pandas.DataFrame: 주어진 단어들의 임베딩\n",
    "        \n",
    "        단어가 임베딩 행렬에 없는 경우 제외\n",
    "    \"\"\"\n",
    "    return embeddings.loc[embeddings.index.intersection(words)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>...</th>\n",
       "      <th>d2491</th>\n",
       "      <th>d2492</th>\n",
       "      <th>d2493</th>\n",
       "      <th>d2494</th>\n",
       "      <th>d2495</th>\n",
       "      <th>d2496</th>\n",
       "      <th>d2497</th>\n",
       "      <th>d2498</th>\n",
       "      <th>d2499</th>\n",
       "      <th>d2500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>including</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standards</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>differs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            d1   d2   d3   d4   d5   d6   d7   d8   d9  d10  ...    d2491  \\\n",
       "including  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "standards  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "differs    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "\n",
       "           d2492  d2493  d2494  d2495  d2496  d2497  d2498  d2499  d2500  \n",
       "including    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "standards    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "differs      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 2500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['standards', 'differs', 'including', 'abcc'] # 'abcc'는 행렬에 없음\n",
    "word_embeddings = get_embeddings_for_words(words, emb_nnse)\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_sum` (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_embeddings_sum(target_embs, context_embs):\n",
    "    \"\"\"[Baseline] 주어진 단어 임베딩들의 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    단순 덧셈\n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (덧셈). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    # 임베딩 행렬 복제본에 작업\n",
    "    target_embs = target_embs.copy()\n",
    "    context_embs = context_embs.copy()\n",
    "    \n",
    "    embs = pd.concat([target_embs, context_embs])\n",
    "    return embs.sum(axis=0).values.reshape(1, -1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합성 결과물 shape = (1, #_of_dimensions)\n",
    "compose_embeddings_sum(word_embeddings.iloc[1:], word_embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `compose_embeddings_reactive` (My)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_embeddings_for_words(['bank'], emb_nnse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d240': ['cliff', 'slope', 'ladder', 'slopes', 'hillside'],\n",
       " 'd974': ['river', 'danube', 'confluence', 'dam', 'bridge']}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = get_embeddings_for_words(['tree', 'river', 'water'], emb_nnse)\n",
    "\n",
    "res = compose_embeddings_reactive(target, context)\n",
    "dims = res.nonzero()[1]\n",
    "explain_dims(dims, emb_nnse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d146': ['bank', 'banking', 'atms', 'atm', 'banks'],\n",
       " 'd1245': ['economic', 'keynesian', 'imf', 'monetary', 'economics'],\n",
       " 'd1393': ['paulson', 'bailout', 'lehman', 'fannie', 'freddie'],\n",
       " 'd1540': ['ngos', 'ict', 'secretariat', 'programme', 'unicef'],\n",
       " 'd1585': ['debit', 'credit', 'payment', 'paypal', 'card'],\n",
       " 'd2191': ['volunteer', 'donate', 'volunteers', 'donation', 'volunteering'],\n",
       " 'd2236': ['coin', 'coins', 'nickels', 'minted', 'krause']}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = get_embeddings_for_words(['money', 'debt', 'finance'], emb_nnse)\n",
    "\n",
    "res = compose_embeddings_reactive(target, context)\n",
    "dims = res.nonzero()[1]\n",
    "explain_dims(dims, emb_nnse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    \"\"\"\n",
    "    Calculate the softmax for the give inputs (array)\n",
    "    :param inputs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.exp(inputs) / float(sum(np.exp(inputs)))\n",
    "\n",
    "def get_sig_dims(emb, thres=0.0):\n",
    "    \"\"\"값이 `thres` 이상인 차원 index 반환\n",
    "    \n",
    "    Args:\n",
    "        emb (numpy.array): 차원을 추출할 임베딩. 1d array.\n",
    "    Returns:\n",
    "        numpy.array: sig 차원이 표시된 mask (예: array([False, True, False, ...]))\n",
    "    \"\"\"\n",
    "    if len(emb.shape) > 1:\n",
    "        raise ValueError('`emb` argument should be 1D array')\n",
    "    \n",
    "    return np.where((emb > thres) == True)[0]\n",
    "\n",
    "def compose_embeddings_reactive(target_embs, context_embs):\n",
    "    \"\"\"[Proposing] 주어진 단어 임베딩들의 contextualized 합을 `pandas.DataFrame` 형태로 반환.\n",
    "    \n",
    "    1. \n",
    "    2. \n",
    "    3. \n",
    "    \n",
    "    Args:\n",
    "        target_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "        context_embs (pandas.DataFrame): 합성할 단어 임베딩. `get_embeddings_for_words`의 반환값\n",
    "    Returns:\n",
    "        pandas.DataFrame: 합성된 단어 임베딩 (Contextualized). shape은 (1, #_of_dimensions)\n",
    "    \"\"\"\n",
    "    # 임베딩 행렬 복제본에 작업 (원본 행렬 유지)\n",
    "    target_embs = target_embs.copy()\n",
    "    context_embs = context_embs.copy()\n",
    "    \n",
    "    # 임베딩을 1차원 벡터로 변환 (context는 먼저 합친 후 변환)\n",
    "    target = target_embs.values.flatten()\n",
    "    context = context_embs.sum().values.flatten()\n",
    "    \n",
    "    #  target에서 0이 아닌 차원 파악\n",
    "    target_dims = target.nonzero()[0]\n",
    "    # context의 sig 차원 파악\n",
    "    context_sig_dims = get_sig_dims(context)\n",
    "    \n",
    "    # 보존할 차원 파악\n",
    "    dims_to_preserve = np.intersect1d(target_dims, context_sig_dims)\n",
    "    \n",
    "    # 디버깅\n",
    "#     print('Target:', target_embs.index.tolist())\n",
    "#     print('Context:', context_embs.index.tolist())\n",
    "#     explain = explain_dims(dims_to_preserve, emb_nnse)\n",
    "#     pprint(explain)\n",
    "\n",
    "    # target에서 보존할 차원 제외한 나머지는 비활성화\n",
    "    contextual = target.copy()\n",
    "    for i in range(0, len(contextual)):\n",
    "        if i not in dims_to_preserve:\n",
    "            contextual[i] = 0.0\n",
    "    \n",
    "    # 반환\n",
    "#     contextual /= contextual.sum()\n",
    "    return contextual.reshape(1, -1)\n",
    "\n",
    "#     # context에서 0이 아닌 차원을 값이 큰 순으로 정렬한 후, 상위 top_dim_k 만큼만 파악.\n",
    "#     # top_dim_k은 차원_갯수*top_dim_ratio\n",
    "#     top_dim_k = int(len(target_embs.columns) * top_dim_ratio)\n",
    "#     context_top_nonzero = np.flip(np.argsort(context), axis=0)[:top_dim_k]\n",
    "    \n",
    "    # both_nonzero_dims : target_nonzero, context_top_nonzero의 교집합\n",
    "#     both_nonzero_dims = np.intersect1d(target_nonzero, context_top_nonzero)\n",
    "    \n",
    "    # target_embs의 나머지 차원 비활성화\n",
    "#     final_emb = (target_embs.values.flatten() + context.values.flatten()) / 2\n",
    "#     final_emb = target_embs.values.flatten()\n",
    "#     for d in range(0, len(final_emb)):\n",
    "#         if d not in both_nonzero_dims:\n",
    "#             final_emb[d] = 0.0\n",
    "    \n",
    "    # 디버그\n",
    "#     print(top_dim_k)\n",
    "#     print(target_nonzero)\n",
    "#     print(context_top_nonzero)\n",
    "    \n",
    "    # 다음 추가 처리 후 결과 반환\n",
    "    #   1. (1, #_of_dimensions) 형태로 reshape\n",
    "#     final_emb = softmax(final_emb)\n",
    "#     return final_emb.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `explain_dim`, `explain_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_dim(index, embeddings, k=5):\n",
    "    col = 'd{}'.format(index+1)\n",
    "    desc = embeddings.sort_values(by=col, ascending=False).index.tolist()[:k]\n",
    "    return {\n",
    "        col: desc\n",
    "    }\n",
    "\n",
    "def explain_dims(indices, embeddings, k=5):\n",
    "    results = {}\n",
    "    for i in indices:\n",
    "        res = _explain_dim(i, embeddings)\n",
    "        results.update(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d22': ['mode', 'modes', 'slider', 'contrast', 'correction'],\n",
       " 'd33': ['multocida', 'sojae', 'element', 'keywords', 'elements']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_dims([21, 32], emb_nnse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_dataset`\n",
    "![wsd-dataset](./images/wsd-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"테스트 할 데이터셋을 `pandas.DataFrame` 형태로 로드\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 데이터셋 경로\n",
    "    Returns:\n",
    "        pandas.DataFrame: 로드한 데이터셋 DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexeme</th>\n",
       "      <th>Target Sense Definition</th>\n",
       "      <th>Target Sense Sentence</th>\n",
       "      <th>Example Definition Sense 1</th>\n",
       "      <th>Example Sentence Sense 1</th>\n",
       "      <th>Example Definition Sense 2</th>\n",
       "      <th>Example Sentence Sense 2</th>\n",
       "      <th>Data Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>Uncertainty prevails in the shadows of the Ira...</td>\n",
       "      <td>used in reference to proximity, ominous oppres...</td>\n",
       "      <td>We've lived in the shadow of the seven-inch si...</td>\n",
       "      <td>a weak or inferior remnant or version of somet...</td>\n",
       "      <td>She had realized then, as she realized now, th...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bond</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Each union must buy an insurance bond to prote...</td>\n",
       "      <td>an insurance policy held by a company, which p...</td>\n",
       "      <td>Insurance company capital-protected guaranteed...</td>\n",
       "      <td>an agreement with legal force, in particular:</td>\n",
       "      <td>In this case, the defendant, a dyer, had given...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>We cannot rule out the possibility of a conspi...</td>\n",
       "      <td>a period of time following the moment of speak...</td>\n",
       "      <td>Pat plans to release a further single in the n...</td>\n",
       "      <td>contracts for assets (especially commodities o...</td>\n",
       "      <td>Then as soon as the cash market closed, the S&amp;...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>So the true question of objective knowledge is...</td>\n",
       "      <td>true, justified belief; certain understanding,...</td>\n",
       "      <td>As a rationalist, he believed that the only pa...</td>\n",
       "      <td>the sum of what is known</td>\n",
       "      <td>He does experimental and anthropological resea...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>art</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>If the show can be taken as a barometer of vis...</td>\n",
       "      <td>the expression or application of human creativ...</td>\n",
       "      <td>A lot of people who know nothing about art say...</td>\n",
       "      <td>works produced by human creative skill and ima...</td>\n",
       "      <td>The most economical way to sum it all up is wi...</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lexeme                            Target Sense Definition  \\\n",
       "0     shadow  used in reference to proximity, ominous oppres...   \n",
       "1       bond  an insurance policy held by a company, which p...   \n",
       "2     future  a period of time following the moment of speak...   \n",
       "3  knowledge  true, justified belief; certain understanding,...   \n",
       "4        art  the expression or application of human creativ...   \n",
       "\n",
       "                               Target Sense Sentence  \\\n",
       "0  Uncertainty prevails in the shadows of the Ira...   \n",
       "1  Each union must buy an insurance bond to prote...   \n",
       "2  We cannot rule out the possibility of a conspi...   \n",
       "3  So the true question of objective knowledge is...   \n",
       "4  If the show can be taken as a barometer of vis...   \n",
       "\n",
       "                          Example Definition Sense 1  \\\n",
       "0  used in reference to proximity, ominous oppres...   \n",
       "1  an insurance policy held by a company, which p...   \n",
       "2  a period of time following the moment of speak...   \n",
       "3  true, justified belief; certain understanding,...   \n",
       "4  the expression or application of human creativ...   \n",
       "\n",
       "                            Example Sentence Sense 1  \\\n",
       "0  We've lived in the shadow of the seven-inch si...   \n",
       "1  Insurance company capital-protected guaranteed...   \n",
       "2  Pat plans to release a further single in the n...   \n",
       "3  As a rationalist, he believed that the only pa...   \n",
       "4  A lot of people who know nothing about art say...   \n",
       "\n",
       "                          Example Definition Sense 2  \\\n",
       "0  a weak or inferior remnant or version of somet...   \n",
       "1      an agreement with legal force, in particular:   \n",
       "2  contracts for assets (especially commodities o...   \n",
       "3                           the sum of what is known   \n",
       "4  works produced by human creative skill and ima...   \n",
       "\n",
       "                            Example Sentence Sense 2 Data Source  \n",
       "0  She had realized then, as she realized now, th...      Oxford  \n",
       "1  In this case, the defendant, a dyer, had given...      Oxford  \n",
       "2  Then as soon as the cash market closed, the S&...      Oxford  \n",
       "3  He does experimental and anthropological resea...      Oxford  \n",
       "4  The most economical way to sum it all up is wi...      Oxford  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드 예시\n",
    "f = '../data/sense-2017/all/2_senses_dev_Noun.csv'\n",
    "dataset = load_dataset(f)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 로드\n",
    "emb_word2vec = load_word2vec()\n",
    "emb_nnse = load_nnse()\n",
    "\n",
    "# spaCy 모델(문맥 단어 추출에 사용) 로드\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_word_pos(test_case):\n",
    "    \"\"\"테스트 케이스 명에서 Target word의 품사가 무엇인지 판단\n",
    "    \n",
    "    제안하는 방법은 (명사, 동사, 형용사) 중 target word의 품사를 제외한 나머지 품사만을 문맥 단어로 활용함. \n",
    "    \n",
    "    Args:\n",
    "        test_case (str): 테스트 케이스 이름 (e.g. \"2_senses_dev_Adjective\")\n",
    "    Returns:\n",
    "        str: 'NOUN', 'VERB', 'ADJ' 중 하나 (spacy의 태그 형식)\n",
    "    \"\"\"\n",
    "    if 'Noun' in test_case:\n",
    "        return 'NOUN'\n",
    "    elif 'Verb' in test_case:\n",
    "        return 'VERB'\n",
    "    elif 'Adjective' in test_case:\n",
    "        return 'ADJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2_senses_dev_Adjective]\n",
      "sum_word2vec: \t0.7121 (47 / 66)\n",
      "sum_nnse: \t0.5909 (39 / 66)\n",
      "reactive_nnse: \t0.9242 (61 / 66)\n",
      "\n",
      "[2_senses_dev_Noun]\n",
      "sum_word2vec: \t0.7574 (128 / 169)\n",
      "sum_nnse: \t0.6686 (113 / 169)\n",
      "reactive_nnse: \t0.9822 (166 / 169)\n",
      "\n",
      "[2_senses_dev_Verb]\n",
      "sum_word2vec: \t0.7120 (89 / 125)\n",
      "sum_nnse: \t0.6480 (81 / 125)\n",
      "reactive_nnse: \t0.9600 (120 / 125)\n",
      "\n",
      "[2_senses_test_Adjective]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    # 데이터셋 파일 목록 읽어오기\n",
    "    files = glob('../data/sense-2017/all/*.csv')\n",
    "    files = [f for f in files if '2_senses' in f]\n",
    "#     files = files[5:6]\n",
    "    \n",
    "    # 각 데이터셋 별로\n",
    "    corrects_all = {}\n",
    "    debugs_all = {}\n",
    "    for f in files:\n",
    "        test_case = os.path.basename(f).split('.')[0]\n",
    "        print('[{}]'.format(test_case))\n",
    "\n",
    "        # 데이터셋 로드\n",
    "        dataset = load_dataset(f)\n",
    "\n",
    "        # 1. 데이터셋 각 row 별로,\n",
    "        corrects_testcase = defaultdict(list) # row 별로 점답 여부 저장 \n",
    "        debugs_testcase = defaultdict(list)   # row 별로 디버그 정보 저장 (합성 벡터, Option 별 점수 등)\n",
    "        for _, row in dataset.iterrows():\n",
    "            # 2. Target 단어 및 Sentence(Target, Option #) 추출\n",
    "            #     sentences[0] = target-sentence, sentences[1:] = option-sentences\n",
    "            target_word = row['Lexeme'].strip()                \n",
    "            sentences = [row[col].strip() for col in row.keys() if 'Sentence' in col]\n",
    "\n",
    "            # 3. Target 단어 및 context 단어들의 임베딩 확보 (`emb_word2vec`, `emb_nnse`)\n",
    "            embeddings = {\n",
    "                'word2vec': {\n",
    "#                     'target': None,\n",
    "#                     'contexts': [None, None]\n",
    "                },\n",
    "                'nnse': {\n",
    "#                     'target': None,\n",
    "#                     'contexts': [None, None]\n",
    "                }\n",
    "            }\n",
    "            ## word2vec 임베딩 확보\n",
    "            # sentences에서 context 단어 추출 (NOUN, VERB, ADJ)\n",
    "            context_pos = {'NOUN', 'VERB', 'ADJ'}\n",
    "            contexts = [extract_context_words(sent, nlp, context_pos) for sent in sentences]\n",
    "            embeddings['word2vec']['target'] = get_embeddings_for_words([target_word], emb_word2vec)\n",
    "            embeddings['word2vec']['contexts'] = [get_embeddings_for_words(cxt, emb_word2vec) for cxt in contexts]\n",
    "\n",
    "            ## NNSE 임베딩 확보\n",
    "            # sentences에서 context 단어 추출\n",
    "            # (NOUN, VERB, ADJ) 중 Target word 품사 제외한 나머지 (예: Target word가 NOUN -> (VERB, ADJ))\n",
    "#                 target_word_pos = get_target_word_pos(test_case)\n",
    "#                 context_pos.remove(target_word_pos)\n",
    "            contexts = [extract_context_words(sent, nlp, context_pos) for sent in sentences]\n",
    "            embeddings['nnse']['target'] = get_embeddings_for_words([target_word], emb_nnse)\n",
    "            embeddings['nnse']['contexts'] = [get_embeddings_for_words(cxt, emb_nnse) for cxt in contexts]\n",
    "\n",
    "            # 5. 임베딩 유형 & 합성 방식 별 성능 비교 (word2vec vs. NNSE & sum vs. reactive)\n",
    "            #     (target & contexts[0]) VS (target & contexts[1]), (target & contexts[2]), ...\n",
    "            #     여기서 contexts[0]는 \"Target Sentence\"에서 추출된 문맥 임베딩, 나머지는 \"Option Sentence\"\n",
    "            ## 합성 방식 별 비교\n",
    "            compose_methods = {\n",
    "                'sum': compose_embeddings_sum,\n",
    "                'reactive': compose_embeddings_reactive\n",
    "            }\n",
    "            for compose_type in ['sum', 'reactive']:\n",
    "                compose_method = compose_methods[compose_type]\n",
    "\n",
    "                ## 임베딩 유형별 비교\n",
    "                emb_types = ['word2vec', 'nnse'] if compose_type == 'sum' else ['nnse']\n",
    "                for emb_type in emb_types:\n",
    "                    # 주어진 유형의 임베딩 확보\n",
    "                    emb_sets = embeddings[emb_type]\n",
    "\n",
    "                    # 실험 프로세스\n",
    "                    setting = '{}_{}'.format(compose_type, emb_type)  # 현재 설정 (포멧: \"합성방법_임베딩유형\")\n",
    "\n",
    "                    if emb_sets['target'].size == 0:  # Target 단어가 임베딩 행렬에 없을 경우 생략\n",
    "                        continue\n",
    "                    else:                             # Target 단어가 있을 경우\n",
    "                        # Target 합성 임베딩 계산\n",
    "                        target_composed = compose_method(emb_sets['target'], emb_sets['contexts'][0])\n",
    "                        # Option 합성 임베딩 계산\n",
    "                        options_composed = [compose_method(emb_sets['target'], cxt) for cxt in emb_sets['contexts'][1:]]\n",
    "\n",
    "                        # Target과 Option 간 cosine similarity 점수 계산 & Option 별 점수 랭킹 계산\n",
    "                        # Option 1이 항상 정답임\n",
    "                        try:\n",
    "                            option_scores = [cosine_similarity(target_composed, opt)\n",
    "                                         for opt in options_composed]\n",
    "                            # 정답 여부 확인\n",
    "                            correct = (np.argmax(option_scores) == 0\n",
    "                                          and option_scores[0].flatten()[0] != 0)\n",
    "                        except ValueError: # NNSE의 경우 context 단어 벡터를 하나도 찾지 못하는 경우가 생김\n",
    "    #                         print('Error:', target_word, setting)\n",
    "                            # 틀렸다고 간주\n",
    "                            correct = False\n",
    "                        # 결과 저장 (row)\n",
    "                        ## 정답 여부 저장\n",
    "                        corrects_testcase[setting].append(correct)\n",
    "                        ## 디버그 정보 저장\n",
    "                        debug = {\n",
    "                            'target_composed': target_composed,\n",
    "                            'options_composed': options_composed,\n",
    "                            'option_scores': option_scores\n",
    "                        }\n",
    "                        debugs_testcase[setting].append(debug)  \n",
    "\n",
    "            # 결과 저장 (test case)\n",
    "            ## 정답 여부 저장\n",
    "            corrects_all[test_case] = corrects_testcase\n",
    "            ## 디버그 정보 저장\n",
    "            debugs_all[test_case] = debugs_testcase\n",
    "\n",
    "        # 점수 출력 (per testcase)\n",
    "        for setting, corrects in corrects_testcase.items():\n",
    "            n_correct = sum(corrects)\n",
    "            n_all = len(corrects)\n",
    "            final_score = n_correct / n_all\n",
    "            print('{}: \\t{:.4f} ({} / {})'.format(setting, final_score, n_correct, n_all))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugs_all['2_senses_dev_Adjective']['reactive_nnse'][0]['option_scores'][0].flatten()[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
